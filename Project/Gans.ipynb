{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33accb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from modulefinder import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d055920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11526475",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/chest-xray-pneumonia /kaggle/working/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /kaggle/working/chest-xray-pneumonia/ -name \"._*\" -delete\n",
    "!find /kaggle/working/chest-xray-pneumonia/ -name \"__MACOSX\" -type d -exec rm -rf {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d530bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder('/kaggle/working/chest-xray-pneumonia/chest_xray/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorModel(nn.Module):\n",
    "    def __init__(self, z_dim=100, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(n_classes, 50)\n",
    "        self.label_dense = nn.Linear(50, 4096)   \n",
    "\n",
    "        self.z_dense = nn.Linear(z_dim, 4096)    \n",
    "        \n",
    "        self.deconv_blocks = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        labels = labels.long()\n",
    "        label_embed = self.embedding(labels)\n",
    "        x_label = self.label_dense(label_embed)\n",
    "        x_label = x_label.view(-1, 64, 8, 8)\n",
    "\n",
    "        x_z = self.z_dense(z)\n",
    "        x_z = x_z.view(-1, 64, 8, 8)\n",
    "\n",
    "        x = torch.cat([x_label, x_z], dim=1)  \n",
    "        x = self.deconv_blocks(x)             \n",
    "        x = self.out_conv(x)                  \n",
    "        return self.tanh(x)\n",
    "\n",
    "\n",
    "class DiscriminatorModel(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(n_classes, 50)\n",
    "        self.label_dense = nn.Linear(50, 256 * 256)  \n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(2, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(8 * 8 * 64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, image, labels, return_features=False):\n",
    "        labels = labels.long()\n",
    "        label_embed = self.embedding(labels)\n",
    "        y = self.label_dense(label_embed).view(-1, 1, 256, 256)\n",
    "\n",
    "        x = torch.cat([image, y], dim=1)      \n",
    "\n",
    "        features = self.conv_blocks(x)        \n",
    "        flat_features = self.flatten(features)  \n",
    "        flat_features = self.drop(flat_features)\n",
    "\n",
    "        out = self.fc(flat_features)\n",
    "        validity = self.sigmoid(out)\n",
    "\n",
    "        if return_features:\n",
    "            return validity, flat_features\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ec495",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "label_dim = 2\n",
    "image_size = 256\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "lr = 0.0001\n",
    "lambda_gp = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.BCELoss()\n",
    "generator = GeneratorModel(z_dim, label_dim).to(device)\n",
    "discriminator = DiscriminatorModel().to(device)\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa529e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, labels) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "\n",
    "        z = torch.randn(batch_size, z_dim, device=device)\n",
    "        fake_images = generator(z, labels).detach()\n",
    "\n",
    "        real_validity, real_features = discriminator(real_images, labels, return_features=True)\n",
    "        fake_validity, _ = discriminator(fake_images, labels, return_features=True)\n",
    "\n",
    "        real_targets = torch.ones_like(real_validity, device=device)\n",
    "        fake_targets = torch.zeros_like(fake_validity, device=device)\n",
    "\n",
    "        d_loss_real = criterion(real_validity, real_targets)\n",
    "        d_loss_fake = criterion(fake_validity, fake_targets)\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "\n",
    "        z = torch.randn(batch_size, z_dim, device=device)\n",
    "        fake_images = generator(z, labels)\n",
    "        fake_validity, fake_features = discriminator(fake_images, labels, return_features=True)\n",
    "\n",
    "        feature_loss = torch.mean((real_features.detach().mean(0) - fake_features.mean(0))**2)\n",
    "\n",
    "        g_loss = feature_loss\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"[{epoch}/{num_epochs}] Step {i} | D_loss: {d_loss.item():.4f} | G_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    if epoch == 0:\n",
    "        z_fixed  = torch.randn(4, z_dim).to(device)\n",
    "        labels_fixed = torch.tensor([0,1,0,1], device=device)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_samples = generator(z_fixed, labels_fixed).cpu()\n",
    "            grid = torchvision.utils.make_grid(fake_samples, nrow=4, normalize=True, padding=2)\n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.title(f\"Epoch {epoch}\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(grid.permute(1,2,0))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import zipfile\n",
    "save_dir = \"/kaggle/working/generated_images\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 🔧 Tham số\n",
    "num_per_label = 50       \n",
    "z_dim = 100             \n",
    "num_classes = 2          \n",
    "\n",
    "def generate_images(generator, z_dim, num_per_label, num_classes, save_dir):\n",
    "    generator.eval()\n",
    "    idx = 0\n",
    "    with torch.no_grad():\n",
    "        for label in range(num_classes):\n",
    "            labels = torch.full((num_per_label,), label, dtype=torch.long, device=next(generator.parameters()).device)\n",
    "            z = torch.randn(num_per_label, z_dim, device=labels.device)\n",
    "            fake_images = generator(z, labels)\n",
    "\n",
    "            for img in fake_images:\n",
    "                save_path = os.path.join(save_dir, f\"label{label}_img{idx:04d}.png\")\n",
    "                save_image(img * 0.5 + 0.5, save_path)  \n",
    "                idx += 1\n",
    "\n",
    "generate_images(generator, z_dim, num_per_label, num_classes, save_dir)\n",
    "\n",
    "zip_path = \"/kaggle/working/generated_images.zip\"\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for root, _, files in os.walk(save_dir):\n",
    "        for file in files:\n",
    "            zipf.write(os.path.join(root, file), file)\n",
    "\n",
    "print(f\"Đã lưu {num_classes*num_per_label} ảnh trong {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
